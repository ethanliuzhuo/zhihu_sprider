# 知乎问题爬虫 ZhihuSpider
知乎回答爬取下载（回答，文章）

顺序是

### 1.先获取可用topic id
`1_get_tid.py`

获取话题id，一个话题下有几十个到一千个（最多）问题
返回得到一个`tid.csv`列表

### 2.获取可用question id
`2_get_qids.py`

获取一个话题topic下的问题列表，删去不可用的问题id
返回得到一个`qid.csv`


### 3.获取该问题的回答

`4_fianl_get_answer.py`

`3_multithereading_get_answer.py`废弃

获取问题下的数据，包括回答，点赞数，回答者，性别，点赞数，留言数，是否追答

在第134行，略去一下冷门问题，比如回答数小于5的略去，可以设置为1，获取所有问题

该版本为多线程
将答案储存到data文件夹里面，一个问题和所有答案储存为json格式

### 4.获取该问题的关键词

`5_get_topic_name.py`
将data文件夹的文件更新


## 要不断地更新ip，否则会失效

### 文章
可以尝试`文章.py`，但是因为是暴力穷举所有文章ID，所有速度很慢
